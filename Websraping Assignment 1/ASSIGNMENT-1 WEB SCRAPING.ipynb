{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d43b03ca",
   "metadata": {},
   "source": [
    "# ASSIGNMENT -1 WEB SCRAPING Fernando Rosales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987fffd9",
   "metadata": {},
   "source": [
    "This resource was very helpfull to get a general idea of Python Webscraping with Python: https://www.youtube.com/watch?v=RvCBzhhydNk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37f0f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from csv import writer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fde179",
   "metadata": {},
   "source": [
    "# 1) Write a program to display all the header tags from Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e56fa0",
   "metadata": {},
   "source": [
    "Solution inspired in the example available at: https://www.w3resource.com/python-exercises/web-scraping/web-scraping-exercise-7.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedfe3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.wikipedia.org/\"\n",
    "page = requests.get(url)\n",
    "print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1172c8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b78c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = soup.find_all(['h1', 'h2','h3','h4','h5','h6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6641722",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('List all the header tags :', *titles, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff38e37",
   "metadata": {},
   "source": [
    "# 2) Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9e08f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd720f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc'\n",
    "page = requests.get(url)\n",
    "print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5620c237",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_movies_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "064bbff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38354a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = soup.find_all('div', class_=\"lister-item mode-advanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5897f65c",
   "metadata": {},
   "source": [
    "* To understand how to extract the text from href tag (TITLE) I used these resources:\n",
    "\n",
    "https://stackoverflow.com/questions/38570411/how-to-scrape-href-with-python-3-5-and-beautifulsoup\n",
    "\n",
    "https://stackoverflow.com/questions/41556848/scraping-a-webpage-for-link-titles-and-urls-utilizing-beautifulsoup\n",
    "\n",
    "https://stackoverflow.com/questions/41577918/a-general-way-to-scrape-link-titles-from-any-site-in-python\n",
    "\n",
    "https://youtu.be/I5L3OJ-xtsw?t=644"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "257ef11d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_15772/253382819.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Usuario\\AppData\\Local\\Temp/ipykernel_15772/253382819.py\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    'year': list.find('span', {class_=\"lister-item-year text-muted unbold\"}).text.replace('\\n', '')\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    " for list in lists:\n",
    "    lists = {\n",
    "    #'ordinal': list.find('span', {class_=\"lister-item-index unbold text-primary\"}).text.replace('\\n', '')                                                                                     \n",
    "    'title': list.find('h3').text.replace('\\n', ',')\n",
    "    'year': list.find('span', {class_=\"lister-item-year text-muted unbold\"}).text.replace('\\n', '') \n",
    "    'rating': list.find('div', {class_=\"inline-block ratings-imdb-rating\"}).text.replace('\\n', '') \n",
    "    }\n",
    "    top_movies_list.append(lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e5328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_100 = pd.DataFrame(lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506ba1cb",
   "metadata": {},
   "source": [
    "# 3) Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26d2430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03c388c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.imdb.com/list/ls009997493/'\n",
    "page = requests.get(url)\n",
    "print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e579cea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8035a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f89f818",
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = soup.find_all('div', class_=\"lister-item mode-detail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1873b755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.', ',1.,Rang De Basanti,(2006),', '(2006)', '8.1']\n",
      "['2.', ',2.,3 Idiots,(2009),', '(2009)', '8.4']\n",
      "['3.', ',3.,Taare Zameen Par,(2007),', '(2007)', '8.3']\n",
      "['4.', ',4.,Dil Chahta Hai,(2001),', '(2001)', '8.1']\n",
      "['5.', ',5.,Swades: We, the People,(2004),', '(2004)', '8.1']\n",
      "['6.', ',6.,Lagaan: Once Upon a Time in India,(2001),', '(2001)', '8.1']\n",
      "['7.', ',7.,Pandillas de Wasseypur,(2012),', '(2012)', '8.2']\n",
      "['8.', ',8.,Barfi!,(2012),', '(2012)', '8.1']\n",
      "['9.', ',9.,Anand,(1971),', '(1971)', '8.1']\n",
      "['10.', ',10.,Munna Bhai M.B.B.S.,(2003),', '(2003)', '8.1']\n",
      "['11.', ',11.,A Wednesday,(2008),', '(2008)', '8.1']\n",
      "['12.', ',12.,Andaz Apna Apna,(1994),', '(1994)', '8']\n",
      "['13.', ',13.,Sholay,(1975),', '(1975)', '8.1']\n",
      "['14.', ',14.,Bhaag Milkha Bhaag,(2013),', '(2013)', '8.2']\n",
      "['15.', ',15.,Hera Pheri,(2000),', '(2000)', '8.1']\n",
      "['16.', ',16.,Udaan,(2010),', '(2010)', '8.1']\n",
      "['17.', ',17.,Kahaani,(2012),', '(2012)', '8.1']\n",
      "['18.', ',18.,Black,(2005),', '(2005)', '8.1']\n",
      "['19.', ',19.,Chak De! India,(2007),', '(2007)', '8.1']\n",
      "['20.', ',20.,Khosla Ka Ghosla!,(2006),', '(2006)', '8.3']\n",
      "['21.', ',21.,Jo Jeeta Wohi Sikandar,(1992),', '(1992)', '8.2']\n",
      "['22.', ',22.,Zindagi Na Milegi Dobara,(2011),', '(2011)', '8.1']\n",
      "['23.', ',23.,Paan Singh Tomar,(2012),', '(2012)', '8.2']\n",
      "['24.', ',24.,Dilwale Dulhania Le Jayenge,(1995),', '(1995)', '8']\n",
      "['25.', ',25.,Omkara,(2006),', '(2006)', '8.1']\n",
      "['26.', ',26.,Lage Raho Munna Bhai,(2006),', '(2006)', '8']\n",
      "['27.', ',27.,Iqbal,(2005),', '(2005)', '8.1']\n",
      "['28.', ',28.,The Lunchbox,(2013),', '(2013)', '7.8']\n",
      "['29.', ',29.,Black Friday,(2004),', '(2004)', '8.4']\n",
      "['30.', ',30.,Company,(2002),', '(2002)', '8']\n",
      "['31.', ',31.,Golmaal,(1979),', '(1979)', '8.5']\n",
      "['32.', ',32.,Dev.D,(2009),', '(2009)', '7.9']\n",
      "['33.', ',33.,Jaane Bhi Do Yaaro,(1983),', '(1983)', '8.3']\n",
      "['34.', ',34.,OMG: Oh My God!,(2012),', '(2012)', '8.1']\n",
      "['35.', ',35.,Mughal-E-Azam,(1960),', '(1960)', '8.1']\n",
      "['36.', ',36.,Gulaal,(2009),', '(2009)', '8']\n",
      "['37.', ',37.,Dor,(2006),', '(2006)', '8']\n",
      "['38.', ',38.,Jab We Met,(2007),', '(2007)', '7.9']\n",
      "['39.', ',39.,Pyaasa,(1957),', '(1957)', '8.3']\n",
      "['40.', ',40.,The Legend of Bhagat Singh,(2002),', '(2002)', '8.1']\n",
      "['41.', ',41.,Masoom,(1983),', '(1983)', '8.4']\n",
      "['42.', ',42.,Salaam Bombay!,(1988),', '(1988)', '7.9']\n",
      "['43.', ',43.,Satya,(1998),', '(1998)', '8.3']\n",
      "['44.', ',44.,Vicky Donor,(2012),', '(2012)', '7.8']\n",
      "['45.', ',45.,Lakshya,(2004),', '(2004)', '7.8']\n",
      "['46.', ',46.,Vaastav: The Reality,(1999),', '(1999)', '8']\n",
      "['47.', ',47.,Kal Ho Naa Ho,(2003),', '(2003)', '7.9']\n",
      "['48.', ',48.,Oye Lucky! Lucky Oye!,(2008),', '(2008)', '7.7']\n",
      "['49.', ',49.,Sarfarosh,(1999),', '(1999)', '8.1']\n",
      "['50.', ',50.,Gangaajal,(2003),', '(2003)', '7.8']\n",
      "['51.', ',51.,Angoor,(1982),', '(1982)', '8.3']\n",
      "['52.', ',52.,Madras Cafe,(2013),', '(2013)', '7.6']\n",
      "['53.', ',53.,English Vinglish,(2012),', '(2012)', '7.8']\n",
      "['54.', ',54.,Chupke Chupke,(1975),', '(1975)', '8.3']\n",
      "['55.', ',55.,Johnny Gaddaar,(2007),', '(2007)', '7.9']\n",
      "['56.', ',56.,Maqbool,(2003),', '(2003)', '8']\n",
      "['57.', ',57.,Hazaaron Khwaishein Aisi,(2003),', '(2003)', '7.9']\n",
      "['58.', ',58.,Rock On!!,(2008),', '(2008)', '7.7']\n",
      "['59.', ',59.,Don,(1978),', '(1978)', '7.7']\n",
      "['60.', ',60.,Chhoti Si Baat,(1976),', '(1976)', '8.3']\n",
      "['61.', ',61.,Guide,(1965),', '(1965)', '8.4']\n",
      "['62.', ',62.,Raanjhanaa,(2013),', '(2013)', '7.6']\n",
      "['63.', ',63.,Deewaar,(1975),', '(1975)', '8']\n",
      "['64.', ',64.,Special Chabbis,(2013),', '(2013)', '8']\n",
      "['65.', ',65.,Padosan,(1968),', '(1968)', '8']\n",
      "['66.', ',66.,Mumbai Meri Jaan,(2008),', '(2008)', '7.7']\n",
      "['67.', ',67.,Ab Tak Chhappan,(2004),', '(2004)', '7.8']\n",
      "['68.', ',68.,Kai po che!,(2013),', '(2013)', '7.7']\n",
      "['69.', ',69.,Awaara,(1951),', '(1951)', '7.8']\n",
      "['70.', ',70.,Shree 420,(1955),', '(1955)', '7.9']\n",
      "['71.', ',71.,Earth,(1998),', '(1998)', '7.6']\n",
      "['72.', ',72.,Gunda,(1998),', '(1998)', '7.3']\n",
      "['73.', ',73.,Parinda,(1989),', '(1989)', '7.8']\n",
      "['74.', ',74.,Dasvidaniya,(2008),', '(2008)', '7.8']\n",
      "['75.', ',75.,Hey Ram,(2000),', '(2000)', '7.9']\n",
      "['76.', ',76.,Pinjar: Beyond Boundaries...,(2003),', '(2003)', '7.9']\n",
      "['77.', ',77.,Socha Na Tha,(2005),', '(2005)', '7.4']\n",
      "['78.', ',78.,Guru,(2007),', '(2007)', '7.7']\n",
      "['79.', ',79.,Bawarchi,(1972),', '(1972)', '8.1']\n",
      "['80.', ',80.,Manorama: Six Feet Under,(2007),', '(2007)', '7.6']\n",
      "['81.', ',81.,Mr. India,(1987),', '(1987)', '7.7']\n",
      "['82.', ',82.,Aamir,(2008),', '(2008)', '7.6']\n",
      "['83.', ',83.,Zakhm,(1998),', '(1998)', '7.9']\n",
      "['84.', ',84.,Water,(I) (2005),', '(I) (2005)', '7.7']\n",
      "['85.', ',85.,Stanley Ka Dabba,(2011),', '(2011)', '7.8']\n",
      "['86.', ',86.,Agneepath,(1990),', '(1990)', '7.6']\n",
      "['87.', ',87.,My Name Is Khan,(2010),', '(2010)', '7.9']\n",
      "['88.', ',88.,Qayamat Se Qayamat Tak,(1988),', '(1988)', '7.4']\n",
      "['89.', ',89.,3 Deewarein,(2003),', '(2003)', '7.8']\n",
      "['90.', ',90.,Abhimaan,(1973),', '(1973)', '7.8']\n",
      "['91.', ',91.,Sarkar,(2005),', '(2005)', '7.6']\n",
      "['92.', ',92.,Bheja Fry,(2007),', '(2007)', '7.6']\n",
      "['93.', ',93.,Mother India,(1957),', '(1957)', '7.8']\n",
      "['94.', ',94.,Jaane Tu... Ya Jaane Na,(2008),', '(2008)', '7.4']\n",
      "['95.', ',95.,Delhi Belly,(2011),', '(2011)', '7.5']\n",
      "['96.', ',96.,Wake Up Sid,(2009),', '(2009)', '7.6']\n",
      "['97.', ',97.,Rangeela,(1995),', '(1995)', '7.4']\n",
      "['98.', ',98.,Shatranj Ke Khilari,(1977),', '(1977)', '7.5']\n",
      "['99.', ',99.,Pyaar Ka Punchnama,(2011),', '(2011)', '7.6']\n",
      "['100.', ',100.,Ek Hasina Thi,(2004),', '(2004)', '7.5']\n"
     ]
    }
   ],
   "source": [
    " for list in lists:\n",
    "    ordinal = list.find('span', class_=\"lister-item-index unbold text-primary\").text.replace('\\n', '')                                                                                     \n",
    "    title = list.find('h3').text.replace('\\n', ',')\n",
    "    year = list.find('span', class_=\"lister-item-year text-muted unbold\").text.replace('\\n', '') \n",
    "    rating = list.find('span', class_=\"ipl-rating-star__rating\").text.replace('\\n', '') \n",
    "    info = [ordinal, title, year, rating]\n",
    "    movies_list.append(lists)\n",
    "    \n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3bcc6c",
   "metadata": {},
   "source": [
    "Arranging Title column: https://datascienceparichay.com/article/pandas-split-column-by-delimiter/#:~:text=Split%20column%20by%20delimiter%20into,True%20to%20the%20expand%20parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33f7cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in headlines:\n",
    "    headlines = {\n",
    "    'title': item.find('a', {'class':'LatestNews-headline'}).text,\n",
    "    'time': item.find('time', {'class':'LatestNews-timestamp'}),\n",
    "    'link': item.find('a', {'class':'LatestNews-headline'})['href'],\n",
    "    }\n",
    "    headlines_list.append(headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cb7bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c798d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfa008a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c64e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cf98e2a",
   "metadata": {},
   "source": [
    "# 5) Write a python program to scrape cricket rankings from icc-cricket.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f388c80",
   "metadata": {},
   "source": [
    "Solution inspired on the code available at: https://medium.com/analytics-vidhya/how-to-scrape-a-table-from-website-using-python-ce90d0cfb607"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6523373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91405765",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "page = requests.get(url)\n",
    "print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8f323a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# parser-lxml = Change html to Python friendly format\n",
    "# Obtain page's information\n",
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ea5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain information from tag <table>\n",
    "table1 = soup.find('div', class_='rankings-block__container full rankings-table')\n",
    "print(table1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941a312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain every title of columns with tag <th>\n",
    "headers = []\n",
    "for i in table1.find_all('th'):\n",
    "    title = i.text\n",
    "    headers.append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3377054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6e2d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe\n",
    "mydata = pd.DataFrame(columns = headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d176365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a for loop to fill mydata\n",
    "for j in table1.find_all('tr')[1:]:\n",
    "    row_data = j.find_all('td')\n",
    "    row = [i.text for i in row_data]\n",
    "    length = len(mydata)\n",
    "    mydata.loc[length] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3464527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = pd.DataFrame(mydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1279223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c3877f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e3f2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72735d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74e3291a",
   "metadata": {},
   "source": [
    "Scraping HTML tables with Pandas: https://www.youtube.com/watch?v=ODNMNwgtehk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4196e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379bdc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_html('https://www.icc-cricket.com/rankings/mens/team-rankings/odi')\n",
    "df = df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682142f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ebca6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c04ead1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89875487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508be430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3d893c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479e2ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5034bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be78c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b0c605",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec65a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded32d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b916c5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82773c37",
   "metadata": {},
   "source": [
    "# 6) Write a python program to scrape cricket rankings from icc-cricket.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c393f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6766c074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90681245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f220914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eca7b2cc",
   "metadata": {},
   "source": [
    "# 7) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f3b0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bba5e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.cnbc.com/world/?region=world%C2%B6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe3a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.67 Safari/537.36',\n",
    "    'accept-language': 'en-US,en;q=0.9,es-419;q=0.8,es-US;q=0.7,es;q=0.6',\n",
    "    'referer': 'https://google.com',\n",
    "    'DNT': '1',\n",
    "    'Fernando': 'subtomeplease',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5852b782",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1cfcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3b0bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348e1c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f83262",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = soup.find_all('li', {'class': 'LatestNews-item'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71135bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(headlines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd72d711",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in headlines:\n",
    "    headlines = {\n",
    "    'title': item.find('a', {'class':'LatestNews-headline'}).text,\n",
    "    'time': item.find('time', {'class':'LatestNews-timestamp'}),\n",
    "    'link': item.find('a', {'class':'LatestNews-headline'})['href'],\n",
    "    }\n",
    "    headlines_list.append(headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ff4c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(headlines_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922f303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b929fb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_headlines = pd.DataFrame(headlines_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a8743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_headlines.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e669e2",
   "metadata": {},
   "source": [
    "# 8) Write a python program to scrape the details of most downloaded articles from AI in last 90 days https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c6bc6",
   "metadata": {},
   "source": [
    "##### To solve this problem:''Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', N one, 10054, None)' \n",
    "These resources were very useful to understand in what the problem consisted, how to solve it and how\n",
    "to apply this solution to subsequent problems:\n",
    "\n",
    "* https://stackoverflow.com/questions/56399462/error-message-10054-when-wescraping-with-requests-module\n",
    "* https://www.youtube.com/watch?v=Oz902cJcCUg (Request Headers for Web Scraping)\n",
    "* https://www.youtube.com/watch?v=Zz4hy4gzQqo (Python Code - Headers for Web Scraping (Scrapy or Requests Dictionary))\n",
    "* https://www.youtube.com/watch?v=m-koIYWCaIo (How Web Scrape Multiple Pages with ONE Function with Python)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e4a1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcc42d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952e3f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.67 Safari/537.36',\n",
    "    'accept-language': 'en-US,en;q=0.9,es-419;q=0.8,es-US;q=0.7,es;q=0.6',\n",
    "    'referer': 'https://google.com',\n",
    "    'DNT': '1',\n",
    "    'Fernando': 'subtomeplease',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9183c4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db1527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d8063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ac5232",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa3a7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = soup.find_all('li', {'class': 'sc-9zxyh7-1 sc-9zxyh7-2 exAXfr jQmQZp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66b66b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84c7ef5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in articles:\n",
    "    articles = {\n",
    "    'title': item.find('h2', {'class':'sc-1qrq3sd-1 MKjKb sc-1nmom32-0 sc-1nmom32-1 hqhUYH ebTA-dR'}).text,\n",
    "    'link': item.find('a', {'class':'sc-5smygv-0 nrDZj'})['href'],\n",
    "    'authors': item.find('span', {'class':'sc-1w3fpd7-0 pgLAT'}).text,\n",
    "    'published_date': item.find('span', {'class':'sc-1thf9ly-2 bKddwo'}).text,\n",
    "    }\n",
    "    articles_list.append(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168b45d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(articles_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4565fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fb9bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles = pd.DataFrame(articles_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c4d5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_articles.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bd4eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles.to_excel('articles.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5033a398",
   "metadata": {},
   "source": [
    "# 9) Write a python program to scrape mentioned details from dineout.co.in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b9f923",
   "metadata": {},
   "source": [
    "Solution heavily inspired on: https://www.youtube.com/watch?v=nCuPv3tf2Hg&list=PL-2ExOOaLQ9AHtQNMBwSE2svDPd0x65Vj&index=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5de7d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de332810",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseurl = \"https://www.dineout.co.in\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "673691b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dfe3ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://www.dineout.co.in/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "488805f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84756861",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_list = soup.find_all('ul', class_='_3LAV0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3fdc2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "locationlinks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e54d5e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.dineout.co.in/delhi', 'https://www.dineout.co.in/mumbai', 'https://www.dineout.co.in/bangalore', 'https://www.dineout.co.in/chennai', 'https://www.dineout.co.in/hyderabad', 'https://www.dineout.co.in/pune', 'https://www.dineout.co.in/kolkata', 'https://www.dineout.co.in/ahmedabad', 'https://www.dineout.co.in/chandigarh', 'https://www.dineout.co.in/goa', 'https://www.dineout.co.in/jaipur', 'https://www.dineout.co.in/lucknow', 'https://www.dineout.co.in/indore', 'https://www.dineout.co.in/udaipur', 'https://www.dineout.co.in/agra', 'https://www.dineout.co.in/vadodara', 'https://www.dineout.co.in/nagpur', 'https://www.dineout.co.in/kochi', 'https://www.dineout.co.in/surat', 'https://www.dineout.co.in/ludhiana']\n"
     ]
    }
   ],
   "source": [
    "for item in location_list:\n",
    "    for link in item.find_all('a', href=True):\n",
    "        locationlinks.append(baseurl + link['href'])\n",
    "\n",
    "print(locationlinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bdb00fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_link = 'https://www.dineout.co.in/pune-restaurants/welcome-back'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "433ccefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restaurants = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b0b33",
   "metadata": {},
   "source": [
    "for link in locationlinks:\n",
    "    r = requests.get(link, headers=headers)\n",
    "    soup = BeautifulSoup(r.content, 'lxml')\n",
    "    for item in restaurantslist:\n",
    "        restaurantslist = {    \n",
    "        'name'= soup.find('a', class_='restnt-name ellipsis'),\n",
    "        #'cuisine': item.find('a', {'class':'stopClickPropagation|w1-restarant'}).,\n",
    "        location= soup.find('div', class_= 'restnt-loc ellipsis'),\n",
    "        ratings= soup.find('div', class_='restnt-rating rating-4'),   \n",
    "        #'image_url': item.find('div', {'class':'img cursor'})\n",
    "        restaurants = {\n",
    "            'name': name,\n",
    "            'location': location,\n",
    "            'ratings': ratings\n",
    "        }\n",
    "        restaurants.append(restaurantslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405b7939",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_p = soup.find_all('div', class_='restnt-main-wrap clearfix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32ca0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in restaurants_p:\n",
    "    restaurants_p = {\n",
    "    'name': item.find('a', {'class':'restnt-name ellipsis'}).text,\n",
    "    #'cuisine': item.find('a', {'class':'stopClickPropagation|w1-restarant'}).,\n",
    "    'location': item.find('div', {'class':'restnt-loc ellipsis'}).text,\n",
    "    'ratings': item.find('div', {'class':'restnt-rating rating-4'}).text,   \n",
    "    #'image_url': item.find('div', {'class':'img cursor'})\n",
    "    }\n",
    "    restaurants_listp.append(restaurants_p)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbb1179",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(restaurants_listp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa9cdfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e73c813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156a01b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cfba4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "596fd369",
   "metadata": {},
   "source": [
    "# 10) Write a python program to scrape the details of top publications from Google Scholar from https://scholar.google.com/citations?view_op=top_venues&hl=en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20562637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedd05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://scholar.google.com/citations?view_op=top_venues&hl=en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda9eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.67 Safari/537.36',\n",
    "    'accept-language': 'en-US,en;q=0.9,es-419;q=0.8,es-US;q=0.7,es;q=0.6',\n",
    "    'referer': 'https://google.com',\n",
    "    'DNT': '1',\n",
    "    'Fernando': 'subtomeplease',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b631b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27250df",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0b1749",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63a5c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain information from tag <table>\n",
    "table2 = soup.find('table', class_='gsc_mp_table')\n",
    "print(table2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef6f772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain every title of columns with tag <th>\n",
    "headers = []\n",
    "for i in table2.find_all('th'):\n",
    "    title = i.text\n",
    "    headers.append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5958405",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1286e099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe\n",
    "mydata2 = pd.DataFrame(columns = headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a93c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a for loop to fill mydata\n",
    "for j in table2.find_all('tr')[1:]:\n",
    "    row_data = j.find_all('td')\n",
    "    row = [i.text for i in row_data]\n",
    "    length = len(mydata2)\n",
    "    mydata2.loc[length] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc417113",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata2 = pd.DataFrame(mydata2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18aa1f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mydata2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
